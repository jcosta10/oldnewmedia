<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<link rel="stylesheet" type="text/css" href="style.css" media="screen" />
	<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@48,400,0,0" />
	<title>BIAS</title>
</head>
<body>
	<div class="nav-right">
		<div class="text-left">
		<a href="discrimination.html" class="text-link-left">Discrimination</a>
		<a href="recogntions.html" class="text-link">Recognition</a>
		<a href="debiasing.html" class="text-link">Debiasing</a>
		</div>
	</div>

	<div class="modle-group">
		<div class="title-button">
		<a class="btn" href="index.html"><span class="material-symbols-outlined">arrow_back</span></a>
		</div>
		<div class="title-button">
		<h1 class="title_h1">Recognition</h1> 
		</div>
	
		<p class="p-left">"Joy Buolamwini and computer scientist Timnit Gebrurevealed that facial recognition technology (FRT) has difficulty identifying the gender of darker-skinned subjects. The problem stems from the libraries on which these algorithms have been traditionally trained: the “ground truth” for these programs are the faces of <span class="container-img">
		<span class="tooltip">Hollywood celebrities
		<span class="image1">
		<img src="Face.png" width="400" height="250">
		<span class="figure">Figura 5. Faces generated using the <br> faces of Hollywood celebrities.<br> Progressive Growing of GANs <a href="https://www.youtube.com/watch?v=XOxxPcy5Gr4">Video</a>
		</span>
		</span>
		</span>
		</span> and university undergraduates, those well-known hotspots of diversity(figure4). At a fundamental level, this “curation” means that ground truth = deep fake."<span class="container-legend"><span class="legend">[3]<span class="legend-2">Chun, Wendy. 2021. Discriminating Data: Correlation, Neighborhoods, and the New Politics of Recognition. Cambridge, Massachusetts: The MIT Press.</span></span></span> </p>
		<p class="p-left">"Keyes’s study of automatic gender detection in facial recognition shows that almost 95 percent of papers in the field treat gender as binary, with the majority describing gender as immutable and physiological. While some might respond that this can be easily remedied by creating more categories, this fails to address the deeper harm of allocating people into gender or race categories without their input or consent. This practice has a long history. Administrative systems for centuries have sought to make humans legible by applying fixed labels and definite properties. The work of essentializing and ordering on the basis of biology or culture has long been used to justify forms of violence and oppression." <span class="container-legend"><span class="legend">[2]<span class="legend-1">Crawford, Kate. 2021 Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence. New Haven, London: Yale University Press.</span></span></span></p>
	</div>

	<div class="nav-left">
		<div class="text-left">
		<a href="bias.html" class="text-link-left">Bias</a>
		<a href="behavior.html" class="text-link-left">
		Behavior</a>
		<a href="errors.html" class="text-link">Errors</a>
		</div>
	</div>
	

</body>
</html>