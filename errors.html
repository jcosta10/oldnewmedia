<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<link rel="stylesheet" type="text/css" href="style.css" media="screen" />
	<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@48,400,0,0" />
	<title>BIAS</title>
</head>
<body>
	<div class="nav-right">
		<div class="text-left">
		<a href="discrimination.html" class="text-link-left">Discrimination</a>
		<a href="recogntions.html" class="text-link">Recognition</a>
		<a href="debiasing.html" class="text-link">Debiasing</a>
		</div>
	</div>

	<div class="modle-group">
		<div class="title-button">
		<a class="btn" href="index.html"><span class="material-symbols-outlined">arrow_back</span></a>
		</div>
		<div class="title-button">
		<h1 class="title_h1">Errors</h1> 
		</div>
	
		<p class="p-left">“…“egregious errors” in terms of the core underlying assumptions: that brain size equated to intelligence, that there are separate human races which are distinct biological species, and that those races could be placed in a hierarchy according to their intellect and innate character. Ultimately, this kind of race science was debunked, but as Cornel West has argued, its dominant metaphors, logics, and categories not only supported white supremacy but also made specific political ideas about race possible while closing down others. “
		<p class="p-left">"…“bias” refers to a type of error that can occur during this predictive process of generalization—namely, a systematic or consistently reproduced <span class="container-img">
		<span class="tooltip">classification error
		<span class="image1">
		<img src="https://www.ft.com/__origami/service/image/v2/images/raw/https%3A%2F%2Fd1e00ek4ebabms.cloudfront.net%2Fproduction%2F1b42fce6-1580-4a66-8908-43f4d852372c.jpg?dpr=1&fit=scale-down&source=next&width=700" width="400" height="250">
		<span class="figure">Figura 3. A skull from Samuel Morton’s <br>cranial collection marked ‘lunatic’. <br>Kate Crawford
		</span>
		</span>
		</span>
		</span> that the system exhibits when presented with new examples." <span class="container-legend"><span class="legend">[2]<span class="legend-1">Crawford, Kate. 2021 Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence. New Haven, London: Yale University Press.</span></span></span>
	</div>

	<div class="nav-left">
		<div class="text-left">
		<a href="bias.html" class="text-link-left">Bias</a>
		<a href="behavior.html" class="text-link-left">
		Behavior</a>
		<a href="errors.html" class="text-link">Errors</a>
		</div>
	</div>

</body>
</html>