<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<link rel="stylesheet" type="text/css" href="style.css" media="screen" />
	<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@48,400,0,0" />
	<title>BIAS</title>
</head>
<body>
	<div class="nav-right">
		<div class="text-left">
		<a href="discrimination.html" class="text-link-left">Discrimination</a>
		<a href="recogntions.html" class="text-link">Recognition</a>
		<a href="debiasing.html" class="text-link">Debiasing</a>
		</div>
	</div>

	<div class="modle-group">
		<div class="title-button">
		<a class="btn" href="index.html"><span class="material-symbols-outlined">arrow_back</span></a>
		</div>
		<div class="title-button">
		<h1 class="title_h1">Debiasing</h1> 
		</div>
	
		<p class="p-left">“To better understand the limitations of analyzing AI bias, we can look to the attempts to fix it. In 2019,<span class="container-legend"><span class="legend">IBM<span class="legend-0">"IBM’s researchers go on to state an even more problematic conclusion: “Aspects of our heritage—including race, ethnicity, culture, geography—and our individual identity—age, gender and visible forms of self-expression—are reflected in our faces.” This claim goes against decades of research that has challenged the idea that race, gender, and identity are biological categories at all but are better understood as politically, culturally, and socially constructed. Embedding identity claims in technical systems as though they are facts observable from the face is an example of what Simone Browne calls “digital epidermalization,” the imposition of race on the body. Browne defines this as the exercise of power when the disembodied gaze of surveillance technologies“ do the work of alienating the subject by producing a ‘truth’ about the body and one’s identity (or identities) despite the subject’s claims.” The foundational problems with IBM’s approach to classifying diversity grow out of this kind of centralized production of identity, led by the machine learning techniques that were available to the team. Skin color detection is done because it can be, not because it says anything about race or produces a deeper cultural understanding. Similarly, the use of cranial measurement is done because it is a method that can be done with machine learning. The affordances of the tools become the horizon of truth. The capacity to deploy cranial measurement sand digital epidermalization at scale drives a desire to find meaning in these approaches, even if this method has nothing to do with culture, heritage, or diversity. They are used to increase a problematic understanding of accuracy. Technical claims about accuracy and performance are commonly shot through with political choices about categories and norms but are rarely acknowledged as such. These approaches are grounded in an ideologica lpremise of biology as destiny, where our faces be come our fate."</span></span></span> tried to respond to concerns about bias in its AI systems by creating what the company described as a more “inclusive” dataset called Diversity in Faces (DiF). <span class="container-img">
		<span class="tooltip">DiF
		<span class="image1">
		<img src="https://www.ibm.com/blogs/research/wp-content/uploads/2019/02/DiF-cropped2.jpg" width="400" height="250">
		<span class="figure">Figura 6. Gender Shades (2018) <br> cited in the Diversity in Faces. <br>IMB Reaserch Blog
		</span>
		</span>
		</span>
		</span>  was part of an industry response to the groundbreaking work released a year earlier by researchers Joy Buolamwini and Timnit Gebru that had demonstrated that several facial recognition systems—including hose by IBM, Microsoft, and Amazon—had far greater error rates for people with darker skin, particularly women. As a result, efforts were on going inside all three companies to show progress on rectifying the problem.
		" <span class="container-legend"><span class="legend">[2]<span class="legend-1">Crawford, Kate. 2021 Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence. New Haven, London: Yale University Press. </span></span></span> </p>
	</div>

	<div class="nav-left">
		<div class="text-left">
		<a href="bias.html" class="text-link-left">Bias</a>
		<a href="behavior.html" class="text-link-left">
		Behavior</a>
		<a href="errors.html" class="text-link">Errors</a>
		</div>
	</div>

</body>
</html>